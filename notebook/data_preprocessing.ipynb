{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -r requirement.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as gen\n",
    "from gensim.models import KeyedVectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and save pre-trained Word2Vec model\n",
    "word2vec_model = gen.load(\"word2vec-google-news-300\")\n",
    "word2vec_model.save(\"/home/mukesh/pikachu/classification_nlp/emb_model/word2vec_model.bin\")\n",
    "\n",
    "\n",
    "\n",
    "# Load and save the pre-trained GloVe model\n",
    "glove_model = gen.load(\"glove-wiki-gigaword-300\")\n",
    "glove_model.save(\"/home/mukesh/pikachu/classification_nlp/emb_model/glove_model.bin\")\n",
    "\n",
    "\n",
    "# Load and save the pre-trained FastText model\n",
    "fasttext_model = gen.load(\"fasttext-wiki-news-subwords-300\")\n",
    "fasttext_model.save(\"/home/mukesh/pikachu/classification_nlp/emb_model/fasttext_model.bin\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "df_pos = open(\"data/Train.pos\", \"r\", encoding=\"latin-1\").read()\n",
    "df_neg = open(\"data/Train.neg\", \"r\", encoding=\"latin-1\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lists for positive and negative sentences\n",
    "df_pos_list = [i for i in df_pos.split(\"\\n\") if len(i) >= 2]\n",
    "df_neg_list = [i for i in df_neg.split(\"\\n\") if len(i) >= 2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_word2vec_model = KeyedVectors.load(\"emb_model/word2vec_model.bin\")\n",
    "loaded_glove_model = KeyedVectors.load(\"emb_model/glove_model.bin\")\n",
    "loaded_fasttext_model = KeyedVectors.load(\"emb_model/fasttext_model.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/mukesh/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lemmatizer and stop words\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contractions = {\n",
    "    \"itâ€™s\": \"it is\", \"it's\": \"it is\", \"don't\": \"do not\", \"i'm\": \"i am\", \"you're\": \"you are\",\n",
    "    \"he's\": \"he is\", \"she's\": \"she is\", \"we're\": \"we are\", \"they're\": \"they are\", \"isn't\": \"is not\",\n",
    "    \"aren't\": \"are not\", \"wasn't\": \"was not\", \"weren't\": \"were not\", \"hasn't\": \"has not\",\n",
    "    \"haven't\": \"have not\", \"hadn't\": \"had not\", \"won't\": \"will not\", \"wouldn't\": \"would not\",\n",
    "    \"can't\": \"cannot\", \"couldn't\": \"could not\", \"shouldn't\": \"should not\", \"mustn't\": \"must not\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_contractions(text):\n",
    "    for contraction, expansion in contractions.items():\n",
    "        text = text.replace(contraction, expansion)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = expand_contractions(text)\n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    tokens = text.split()\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    tokens = [re.sub(r'(.)\\1{2,}', r'\\1', word) for word in tokens]\n",
    "    preprocessed_text = ' '.join(tokens)\n",
    "    return preprocessed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/mukesh/nltk_data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the text data\n",
    "df_pos_preprocessed = [preprocess_text(sentence) for sentence in df_pos_list]\n",
    "df_neg_preprocessed = [preprocess_text(sentence) for sentence in df_neg_list]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame for positive and negative data\n",
    "positive_df = pd.DataFrame({\n",
    "    'original_text': df_pos_list,\n",
    "    'processed_text': df_pos_preprocessed,\n",
    "    'level': 'positive'\n",
    "})\n",
    "\n",
    "negative_df = pd.DataFrame({\n",
    "    'original_text': df_neg_list,\n",
    "    'processed_text': df_neg_preprocessed,\n",
    "    'level': 'negative'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate both DataFrames\n",
    "final_df = pd.concat([positive_df, negative_df], ignore_index=True)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "final_df.to_csv(\"processed_data.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_embedding(sentence, model):\n",
    "    words = sentence.split()\n",
    "    word_embeddings = []\n",
    "    \n",
    "    for word in words:\n",
    "        if word in model:\n",
    "            word_embeddings.append(model[word])\n",
    "        else:\n",
    "            word_embeddings.append(np.zeros(model.vector_size))\n",
    "\n",
    "    # Calculate the mean of the embeddings; if no embeddings, return a zero vector\n",
    "    if len(word_embeddings) == 0:\n",
    "        return np.zeros(model.vector_size)\n",
    "    \n",
    "    return np.mean(word_embeddings, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings for positive and negative sentences\n",
    "w2v_pos_embeddings = [get_sentence_embedding(sentence, loaded_word2vec_model) for sentence in df_pos_preprocessed]\n",
    "w2v_neg_embeddings = [get_sentence_embedding(sentence, loaded_word2vec_model) for sentence in df_neg_preprocessed]\n",
    "\n",
    "glove_pos_embeddings = [get_sentence_embedding(sentence, loaded_glove_model) for sentence in df_pos_preprocessed]\n",
    "glove_neg_embeddings = [get_sentence_embedding(sentence, loaded_glove_model) for sentence in df_neg_preprocessed]\n",
    "\n",
    "fasttext_pos_embeddings = [get_sentence_embedding(sentence, loaded_fasttext_model) for sentence in df_pos_preprocessed]\n",
    "fasttext_neg_embeddings = [get_sentence_embedding(sentence, loaded_fasttext_model) for sentence in df_neg_preprocessed]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Generate embeddings for all sentences in final_df\n",
    "final_df['w2v_embedding'] = final_df['processed_text'].apply(lambda x: get_sentence_embedding(x, loaded_word2vec_model))\n",
    "final_df['glove_embedding'] = final_df['processed_text'].apply(lambda x: get_sentence_embedding(x, loaded_glove_model))\n",
    "\n",
    "final_df['fasttest_embedding'] = final_df['processed_text'].apply(lambda x: get_sentence_embedding(x, loaded_fasttext_model))\n",
    "\n",
    "\n",
    "# # Save the DataFrame with embeddings to a CSV file\n",
    "final_df.to_csv(\"text_embedding.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_text</th>\n",
       "      <th>processed_text</th>\n",
       "      <th>level</th>\n",
       "      <th>w2v_embedding</th>\n",
       "      <th>glove_embedding</th>\n",
       "      <th>fasttest_embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the rock is destined to be the 21st century's ...</td>\n",
       "      <td>rock destined century new conan going make spl...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[0.03640926585477941, 0.06906666475183823, -0....</td>\n",
       "      <td>[0.05452945433995303, -0.07681117634124615, -0...</td>\n",
       "      <td>[-0.0015215861653012004, -0.020201347056118882...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the gorgeously elaborate continuation of \" the...</td>\n",
       "      <td>gorgeously elaborate continuation lord ring tr...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[-0.049177689985795456, 0.008171775124289772, ...</td>\n",
       "      <td>[-0.1680255799490789, -0.07400813410905274, 0....</td>\n",
       "      <td>[0.003546609088185836, -0.01697812804989305, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>effective but too-tepid biopic</td>\n",
       "      <td>effective tootepid biopic</td>\n",
       "      <td>positive</td>\n",
       "      <td>[0.08540852864583333, -0.07674153645833333, -0...</td>\n",
       "      <td>[0.10935333867867787, -0.15938666959603628, 0....</td>\n",
       "      <td>[0.003958086551089461, -0.03526200043658415, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>if you sometimes like to go to the movies to h...</td>\n",
       "      <td>sometimes like go movie fun wasabi good place ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[0.010335286, -0.0048828125, -0.010218303, 0.1...</td>\n",
       "      <td>[-0.030760799, 0.069041885, 0.09241887, -0.071...</td>\n",
       "      <td>[-0.017284378, -0.017631331, 0.014068676, 0.03...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>emerges as something rare , an issue movie tha...</td>\n",
       "      <td>emerges something rare issue movie thats hones...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[0.090576171875, 0.02848229041466346, -0.06612...</td>\n",
       "      <td>[-0.002130763, 0.058085773, 0.05994094, -0.108...</td>\n",
       "      <td>[0.005082369, 0.0033200698, 0.039461907, 0.005...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       original_text  \\\n",
       "0  the rock is destined to be the 21st century's ...   \n",
       "1  the gorgeously elaborate continuation of \" the...   \n",
       "2                     effective but too-tepid biopic   \n",
       "3  if you sometimes like to go to the movies to h...   \n",
       "4  emerges as something rare , an issue movie tha...   \n",
       "\n",
       "                                      processed_text     level  \\\n",
       "0  rock destined century new conan going make spl...  positive   \n",
       "1  gorgeously elaborate continuation lord ring tr...  positive   \n",
       "2                          effective tootepid biopic  positive   \n",
       "3  sometimes like go movie fun wasabi good place ...  positive   \n",
       "4  emerges something rare issue movie thats hones...  positive   \n",
       "\n",
       "                                       w2v_embedding  \\\n",
       "0  [0.03640926585477941, 0.06906666475183823, -0....   \n",
       "1  [-0.049177689985795456, 0.008171775124289772, ...   \n",
       "2  [0.08540852864583333, -0.07674153645833333, -0...   \n",
       "3  [0.010335286, -0.0048828125, -0.010218303, 0.1...   \n",
       "4  [0.090576171875, 0.02848229041466346, -0.06612...   \n",
       "\n",
       "                                     glove_embedding  \\\n",
       "0  [0.05452945433995303, -0.07681117634124615, -0...   \n",
       "1  [-0.1680255799490789, -0.07400813410905274, 0....   \n",
       "2  [0.10935333867867787, -0.15938666959603628, 0....   \n",
       "3  [-0.030760799, 0.069041885, 0.09241887, -0.071...   \n",
       "4  [-0.002130763, 0.058085773, 0.05994094, -0.108...   \n",
       "\n",
       "                                  fasttest_embedding  \n",
       "0  [-0.0015215861653012004, -0.020201347056118882...  \n",
       "1  [0.003546609088185836, -0.01697812804989305, 0...  \n",
       "2  [0.003958086551089461, -0.03526200043658415, -...  \n",
       "3  [-0.017284378, -0.017631331, 0.014068676, 0.03...  \n",
       "4  [0.005082369, 0.0033200698, 0.039461907, 0.005...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
